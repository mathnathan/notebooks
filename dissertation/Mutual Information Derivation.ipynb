{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Theory Derivation\n",
    "\n",
    "Let us begin with the definition of mutual information. \n",
    "\n",
    "![Mutual Information](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy-mutual-information-relative-entropy-relation-diagram.svg/1200px-Entropy-mutual-information-relative-entropy-relation-diagram.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations $x$ come from an unknown data generating distribution $x \\sim P(X)$. The random variable $Y$ represents the weights of a neuron and its pdf is a Gaussian. Because it will be referenced frequently, we denote it with a $q(y)$. The weights of the neuron are updated after being stimulated by samples from the data generating distribution $P$. Therefore there is a dependence between the two and $p(x,y) \\neq p(x)q(y)$. One way to quantify this interdependence with the mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(X;Y) = \\int_{X\\times Y} p(x,y) \\log \\frac{p(x,y)}{p(x)q(y)}d(x,y)$$\n",
    "\n",
    "Fubini-Tonelli's Theorem tell us that as long as $X$ and $Y$ are $\\sigma$-finite measure spaces and $f$ is a measurable function then\n",
    "\n",
    "$${\\displaystyle \\int _{X\\times Y}\\big|f(x,y)\\big|\\,{\\text{d}}(x,y)} = \\int _{X}\\left(\\int _{Y}\\big|f(x,y)\\big|\\,{\\text{d}}y\\right)\\,{\\text{d}}x=\\int _{Y}\\left(\\int _{X}\\big|f(x,y)\\big|\\,{\\text{d}}x\\right)\\,{\\text{d}}y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we break up the integrand using logarithmic properties we can simplify the formula\n",
    "\n",
    "$$I(X;Y) = \\int_X \\int_Y p(x,y) \\log \\frac{p(x,y)}{p(x)}{\\text{d}}y \\,{\\text{d}}x - \\int_Y \\int_X p(x,y) \\log p(y) {\\text{d}}x \\,{\\text{d}}y$$\n",
    "\n",
    "To simplify things futher wesubstitute $p(x,y) = p(y|x)p(x)$ in the left term\n",
    "\n",
    "\\begin{align}\n",
    "I(X;Y) &= \\int_X \\int_Y  p(y|x)p(x) \\log p(y|x){\\text{d}}y \\,{\\text{d}}x - \\int_Y \\bigg( \\int_X p(x,y) {\\text{d}}x \\bigg) \\log p(y) {\\text{d}}y \\\\\n",
    "&= \\int_X p(x) \\left( \\int_Y  p(y|x) \\log p(y|x){\\text{d}}y\\right) \\,{\\text{d}}x - \\int_Y p(y) \\log p(y) {\\text{d}}y \\\\ \n",
    "&= \\mathbb{E}_X \\left[ h(x) \\right] + H(Y)\n",
    "\\end{align}\n",
    "\n",
    "where $h(x)$ is the negative entropy of the conditional distribution $P(Y|X=x )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **To be done in the future**: Show that $h(x) \\approx q(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that $h(x) \\approx q(x)$...\n",
    "\n",
    "\\begin{align}\n",
    "q(\\boldsymbol{x} | \\theta) &= \\frac{1}{\\sqrt{(2\\pi)^{k} \n",
    "|\\Sigma|}} \\exp \\bigg\\{ -\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}\n",
    "(\\boldsymbol{x}-\\boldsymbol{\\mu}) \\bigg \\} \\\\\n",
    "&= \\frac{1}{C}\\exp \\{ -\\frac{1}{2}d^2(\\boldsymbol{x}, \\boldsymbol{\\mu};\\Sigma)\\}\n",
    "\\end{align}\n",
    "\n",
    "where $C=\\sqrt{(2\\pi)^{k} |\\Sigma|}$ is the volume under an unnormalized Gaussian and $d(\\boldsymbol{x}, \\boldsymbol{\\mu};\\Sigma)$ is the mahalanobis distance with respect to covariance matrix $\\Sigma$.\n",
    "\n",
    "Our objective is to learn a set of parameters $\\theta$ that maximize the mutual information $I(X;Y)$, so we represent this as an objective function in terms of the parameters\n",
    "\n",
    "\\begin{align}\n",
    "f(\\mu,\\Sigma) &= \\mathbb{E}_X [q(\\boldsymbol{x} | \\mu, \\Sigma) ] + H[q(\\boldsymbol{x} | \\mu, \\Sigma)] \\\\ \n",
    "\\end{align}\n",
    "\n",
    "In addition we can calculate the differential entropy of the multivariate gaussian pdf for $Y$ analytically and substitute it into our objective.\n",
    "\n",
    "\\begin{align}\n",
    "H[q(\\boldsymbol{x} | \\mu, \\Sigma)] &= \\frac{1}{2}\\log | 2 \\pi e \\Sigma | \\\\\n",
    "f(\\mu,\\Sigma) &= \\mathbb{E}_X [q(\\boldsymbol{x} | \\mu, \\Sigma) ] + \\frac{1}{2}\\log | 2 \\pi e \\Sigma | \\\\ \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the mutual information we maximize the objective using traditional techniques\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\boldsymbol{\\mu}} &= \\mathbb{E}_X \\big[ \\frac{\\partial q(x|\\boldsymbol{\\mu}, \\Sigma)}{\\partial \\boldsymbol{\\mu}} \\big] \\\\\n",
    "\\frac{\\partial f}{\\partial \\Sigma} &= \\mathbb{E}_X \\big[ \\frac{\\partial q(x|\\boldsymbol{\\mu}, \\Sigma)}{\\partial \\Sigma} \\big] + \\frac{1}{2}\\Sigma^{-1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "We begin by recalling the partials of the multivariate gaussian pdf for both the mean $\\boldsymbol{\\mu}$ and the covariance $\\Sigma$.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial q}{\\boldsymbol{\\mu}} &= q(x|\\boldsymbol{\\mu}, \\Sigma) \\Sigma^{-1} (\\boldsymbol{x}-\\boldsymbol{\\mu}) \\\\\n",
    "\\frac{\\partial q}{\\partial \\Sigma} &= -q(x|\\boldsymbol{\\mu}, \\Sigma) \\frac{1}{2}(\\Sigma^{-1}-\\Sigma^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "(see appendix for derivations)\n",
    "\n",
    "Substituting these back into our gradient terms we arrive at the following for the mean $\\mu$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\boldsymbol{\\mu}} &=  \\mathbb{E}_X \\big[ q(x|\\boldsymbol{\\mu}, \\Sigma)\\Sigma^{-1} (\\boldsymbol{x}-\\boldsymbol{\\mu}) \\big] \\\\\n",
    "&=  \\Sigma^{-1} \\mathbb{E}_X \\big[ q(x|\\boldsymbol{\\mu}, \\Sigma)(\\boldsymbol{x}-\\boldsymbol{\\mu}) \\big] \\\\\n",
    "\\end{align}\n",
    "\n",
    "and below for $\\Sigma$\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\Sigma} &= \\mathbb{E}_X \\big[ -q(x|\\boldsymbol{\\mu}, \\Sigma) \\frac{1}{2}(\\Sigma^{-1}-\\Sigma^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}) \\big] + \\frac{1}{2}\\Sigma^{-1} \\\\\n",
    "&= \\frac{1}{2}\\Sigma^{-1} \\bigg[ \\mathbb{E}_X \\big[q(x|\\boldsymbol{\\mu}, \\Sigma) ((\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^T-\\Sigma) \\big] + \\Sigma \\bigg] \\Sigma^{-1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "If the expectation in the above expression equals zero, then then entire partial derivative is zero. Furthermore, if $\\Sigma$ is invertible then it is the only way the partial can be zero.\n",
    "\n",
    "Lastly we substitute the definition of the Gaussian $q$ from above.\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\mu} &=  \\Sigma^{-1} \\mathbb{E}_X \\big[ q(x|\\boldsymbol{\\mu}, \\Sigma)(\\boldsymbol{x}-\\boldsymbol{\\mu}) \\big] \\\\\n",
    "&=  \\frac{1}{C}\\Sigma^{-1} \\mathbb{E}_X \\big[\\exp \\{ -\\frac{1}{2}d^2(\\boldsymbol{x}, \\boldsymbol{\\mu};\\Sigma) \\}(\\boldsymbol{x}-\\boldsymbol{\\mu}) \\big] \\\\\n",
    "\\end{align}\n",
    "\n",
    "Doing the same for the partial with respect to $\\Sigma$ we arrive at\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial \\Sigma} &= \\frac{1}{2}\\Sigma^{-1} \\bigg[ \\mathbb{E}_X \\big[q(x|\\boldsymbol{\\mu}, \\Sigma) ((\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^T-\\Sigma) \\big] - \\Sigma \\bigg] \\Sigma^{-1} \\\\\n",
    "&= \\frac{1}{2C}\\Sigma^{-1} \\bigg[ \\mathbb{E}_X \\big[\\exp \\{ -\\frac{1}{2}d^2(\\boldsymbol{x}, \\boldsymbol{\\mu};\\Sigma) \\} [(\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^T-\\Sigma] \\big] + \\Sigma\\sqrt{(2\\pi)^{k} |\\Sigma|} \\bigg] \\Sigma^{-1} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Finally we can produce our stochastic approximation update equations...\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_{i+1} &= \\mu_i + \\exp \\{ -\\frac{1}{2}d^2(\\boldsymbol{x}_i, \\boldsymbol{\\mu}_i;\\Sigma_i) \\}(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_i) \\\\\n",
    "\\Sigma_{i+1} & = \\Sigma_i + \\exp \\{ -\\frac{1}{2}d^2(\\boldsymbol{x}_i, \\boldsymbol{\\mu}_i;\\Sigma_i) \\} [(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_i)(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_i)^T-\\Sigma_i] + \\Sigma_i\\sqrt{(2\\pi)^{k} |\\Sigma_i|} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
