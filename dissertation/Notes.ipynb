{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives are the Problem\n",
    "\n",
    "One of the current limitations of AI is the vantage point that an algorithm must \"perform well\" at a given task. This idea came directly from the field of ML and is the fundamental premise in generalization theory. It is a useful and powerful approach to problem solving. It has enabled the field of ML to accomplish many impressive things. But in its narrow interpretation, it may be missing the true spirit of AI.\n",
    "\n",
    "For example, let us consider the vision system. At what point along the vision processing pipeline was the extraction of geometric primitives learned? When was this a 'task' that was learned? Experts in ML will say that those features were learned as a by product of the learning algorithm. Backprop informed the previous layers that extraction of those geometric features were useful in solving down stream tasks. However, there is more evidence against the biological plausability of backprop than for it. If we release our affinity for the traditional ML paradigm of overparameterize -> create objective function -> backprop -> update -> repeat, we may find alternative approaches that are more consistent with observations, and perhaps, bring us closer to AI.\n",
    "\n",
    "### There is No Free Lunch\n",
    "\n",
    "This brings us back to the ever important no free lunch theorem.\n",
    "\n",
    "### The Futility of Bias-Free Learning \n",
    "\n",
    "In [this paper](https://arxiv.org/pdf/1907.06010.pdf) by Montanez they explore the necessity of bias. However, once again, where to the biases come from? That is what I aim to show.\n",
    "\n",
    "### Probably Approximately Correct\n",
    "\n",
    "This work, just like all the others, assumes the existence of something to be learned. Here they call them 'concepts'. Concepts are once again the same as tasks or goals, though is more specific to the data itself. It is a thing that needs to be learned.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "All of these 'problems' in ML and Learning Theory stem from the explicit formulation of a task. However, is there something deeper going on? After all, where do these tasks come from? Is the universe one giant game where agents are successively presented with taks and challenges? The most obvious answer is, no. We create the tasks. So, if we relax the need for specifying tasks and objectives, can we find another approach that gives rise to 'intelligent systems'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "- Begin with a demo of all the previous AI 'hypes' that all shortly collapsed. [The very first AI winter](https://www.youtube.com/watch?v=i8D90DkCLhI&list=PLiaHhY2iBX9ihLasvE8BKnS2Xg8AhY6iV&index=1), [Expert Systems](https://www.youtube.com/watch?v=0cRXaORbIFA&list=PLiaHhY2iBX9ihLasvE8BKnS2Xg8AhY6iV&index=3), [Self-driving cars by 2002](https://www.youtube.com/watch?v=DIAKL0s4Mzs), etc. Then predict that the current [ML hype](https://www.technologyreview.com/s/607970/experts-predict-when-artificial-intelligence-will-exceed-human-performance/) leading to sentient AI, is the same.\n",
    "\n",
    "- The propensity for a neuron to adapt to emulate its input is not what we will call bias. While it is clearly an assumption, and is something that can be defined as bias, we argue that the alternative, not emulating the input, leads to chaos and randomness and is the opposite of intelligence. So, rather than calling it a bias, to differentiate it from what we will later call bias and to emphasize its importance, we consider it to be a fundamental requirement for the creation of intelligent systems. What are biases then? These are random changes in the neurons that have resulted in (more fit) systems. In other words, simply the ones that were able to continue passing on their information. Evolution if you will.\n",
    "\n",
    "- In [\"Adversarial Examples Are Not Bugs, They Are Features\"](http://gradientscience.org/adv/), they argue that these adversarial examples are a property of the deep learning process. I would like to show that they are indeed a feature to Deep Learning, and while they are not a bug to DL, but they are a bug to AI.\n",
    "\n",
    "- Learning rules from data is a fundamentally ill-posed problem. There is no unique answer.\n",
    "\n",
    "- We often use the human ability to learn very quickly (one-shot learning) as an example of how we're missing something in our understanding of intelligence. And the answer is obviously, yes we are. We are missing millions of years of evolution. All of the mechanisms that our brain employs to hold information in our minds, compute on it using bias passed through genetics, and then to act on that knowledge, were finely honed over millions of years. Start with a picture of a human (what an impressive creature), but this is not the true story. Then zoom out and show millions of evolutionary ancestors in the shape of a triangle all the way back to bacteria.\n",
    "\n",
    "- Another thing we need to let go is the need for random sampling. It is true, if our agent is only exposed to cats, then it will never be able to determine what a dog is. However, if our agent is only ever exposed to cats, then why does it need to know what a dog is? We assume that the data being presented by the environment or the (universe) is by definition what is relevant to the agent. Therefore, the biases NATURALLY form and are in fact required. \n",
    "\n",
    "- I argue that concepts, goals, tasks, do not exist outside our collective imagination. The universe is not imposing them, and the learning agent can't be privvy to them if they have not learned them. Therefore, what \"concepts\" or \"tasks\" are being learned? I want to show that they are a product of the environment, and the randomness of the system. THERE ARE NO TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just as Kenneth Stanley's 'novelty search', showed how random incremental convergences lead to much better results than global optimization, my argument, the 'tasks' that are randomly created by the environment and evolution shows how it is better than objective ML. \n",
    "\n",
    "- Evolution is a multi-agent exploratory optimization problem. Imagine a very complex high-dimensional cost surface. Each agent, randomly initialized via evolution, is like a different point on the surface. They can adapt locally and move around the surface. The ones that perform better move lower and survive longer. They then pass their information on to new agents that continue to make local adaptations and improve.\n",
    "\n",
    "- Under the Policy Optimization approach to RL, the evolutionary approach Derivative Free Optimization (DFO) is very similar in spirit to the approaches and techniques used here. [Derivative-free optimization methods](http://www.optimization-online.org/DB_FILE/2019/04/7153.pdf)\n",
    "\n",
    "![](images/rl_map.png)\n",
    "\n",
    "- Another issue is with one of the colloquial definitions of Deep RL. Below is an intuitive definition from a talk by John Schulman (OpenAI). Information flows from the back of the brain to the front. By the time the frontal cortex recieves input it has integrated all of the sensory information from the environment. This includes extraction of all the features of vision, motion detection, object recognition, some form of frequency decomposition of acuostics, smell, touch, etc. So to say that Deep RL is an attempt to \"do whatever the frontal cortex is doing\", is to assume that the brain's feature extraction capacity, finely honed over millions of years, is either irrelevant to the problem, or can be learned via back-propagation. Though perhaps learning what the frontal cortex could do, even on trivial data, is still desireable.\n",
    "\n",
    "![](images/deep_rl_brain.png)\n",
    "\n",
    "- It seems the RL technique called the Cross-Entropy method (CEM) is very similar to your research. For an overview of the general techniques you should skim [Reinforcement Learning Algorithms Quick Overview\n",
    "](https://medium.com/@jonathan_hui/rl-reinforcement-learning-algorithms-quick-overview-6bf69736694d). Furthermore, this simple cross-entropy method has dramatically out performed nearly all other RL techniques on the game of tetris. It is hypothesized that this is simply due to the limited number of features necessary to solve the problem. The CEM works pretty well for low dimensional problems. However, it does not scale well to higher dimensional problems. This is also very similar to the [Minorization-Maximization (MM) algorithm](https://en.wikipedia.org/wiki/MM_algorithm)\n",
    "\n",
    "- You should mention and explore the [Nevergrad library from FAIR](https://engineering.fb.com/ai-research/nevergrad/)\n",
    "\n",
    "- More papers to read [Meta-learning](https://arxiv.org/pdf/1804.00222.pdf)\n",
    "\n",
    "- Learning local updates rules (Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Université de Montréal,\n",
    "Département d’informatique et de recherche opérationnelle, 1990.) and (Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule. In Preprints Conf. Optimality in Artificial and Biological Neural Networks, pages 6–8. Univ. of Texas, 1992.)\n",
    "- Other researchers assuming that neurons attempt to maximize their depolarization [Heterostasis Theory](https://apps.dtic.mil/dtic/tr/fulltext/u2/742259.pdf)\n",
    "- This is also very similar to [Generalized Hebbian Algorithm](https://en.wikipedia.org/wiki/Generalized_Hebbian_Algorithm)\n",
    "- Be sure to discuss the biological implausibility of [Neural Backpropagation](https://en.wikipedia.org/wiki/Neural_backpropagation) and the retrograde signaling required "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Learning Theory\n",
    "\n",
    "It is important to have a thorough understanding of risk minimization from a machine learning perspective.\n",
    "\n",
    "Let $\\Omega$ be a sample space representing stimuli from the environment. Furthermore we assume samples $\\omega \\in \\Omega$ are produced from the environment according to a distribution $D$, the unknown data generating distribution. There is a label function $y_i = f(\\omega_i)$ that assigns a correct label $y_i$ to each example $\\omega_i$. The goal of a learning agent is to approximate the label function $f$. This is a classification problem and our classification error is defined as the probability of misclassifying a random sample $\\omega$ drawn from $D$. In other words, given a classifier $h$ the classification error is \n",
    "\n",
    "$$\\mathcal{L}_{D,f}[h] := P_{\\omega \\sim D}[h(\\omega) \\neq f(\\omega] := D(\\{\\omega : h(\\omega) \\neq f(\\omega)\\})$$\n",
    "\n",
    "$\\mathcal{L}_{D,f}[h]$ has many names such as generalization error, the risk, or the true error of h. The learner does not know $D$ nor $f$. The only information available to the learner is what is called the training set $S = \\{(w_i,y_i)\\}$  $\\forall$  $i \\in [1,\\dots,m]$. Therefore it is not possible to measure $\\mathcal{L}_{D,f}[h]$ directly, instead we use what is called the empirical error or empirical risk.\n",
    "\n",
    "$$L_S[h] = \\frac{\\big| \\{i\\in[1,\\dots,m]\\ : h(\\omega_i) \\neq y_i)\\}\\big|}{m}$$\n",
    "\n",
    "The process of searching for the labeler function $h$ that minimizes the empirical risk $L_S[h]$ is called **Empirical Risk Minimization** or **ERM**. If one is not careful it is easy to choose $h$ such that the empirical risk is zero, but the generalization error is large. This is called over fitting. To mitigate overfitting we want to introduce constraints or inductive biases that constrain the class of functions $h$ we can choose from. Call the constrained set of functions the *hypothesis class* $\\mathcal{H}$.\n",
    "\n",
    "The simplest type of restriction on a class is imposing an upper bound on its size\n",
    "(that is, the number of predictors $h \\in \\mathcal{H}$). In this section, we show that if $\\mathcal{H}$ is\n",
    "a finite class then **ERM**$_\\mathcal{H}$ will not overfit, provided it is based on a sufficiently\n",
    "large training sample (this size requirement will depend on the size of $\\mathcal{H}$)\n",
    "\n",
    "### Difference from Bayesian Networks\n",
    "\n",
    "Density estimation techniques explicitly build sta- tistical models (such as BAYESIAN NETWORKS). However in Bayesian networks each node is a random variable and represents a conditional probability\\\n",
    "\n",
    "![](https://www.bayesserver.com/docs/images/asia-animated.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axioms of Probability\n",
    "\n",
    "- Fuzzy Logic\n",
    "- Three-valued logic\n",
    "- Dempster-Shafer\n",
    "- Non-monotonic reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Learning Theory (Shalev-Shwartz, Shamir, Srebro & Sridharan 2010))\n",
    "\n",
    "- Binary Classification: Let Z = X × {0,1}, let H be a set of functions h : X 7→ {0,1}, and let\n",
    "f(h;(x, y)) = 11{h(x)6=y}\n",
    ". Here, f(·) is simply the 0 − 1 loss function, measuring whether the binary\n",
    "hypothesis h(·) misclassified the example (x, y).\n",
    "- Regression: Let Z = X × Y where X and Y are bounded subsets of R\n",
    "n\n",
    "and R respectively, let H\n",
    "be a set of bounded functions h : X\n",
    "n\n",
    "7→ R, and let f(h;(x, y)) = (h(x)−y)\n",
    "2\n",
    ". Here, f(·) is simply the\n",
    "squared loss function.\n",
    "- Large Margin Classification in a Reproducing Kernel Hilbert Space (RKHS): Let Z = X × {0,1},\n",
    "where X is a bounded subset of an RKHS, let H be another bounded subset of the RKHS, and let\n",
    "f(h;(x, y)) = max{0,1−yhx,hi}. Here, f(·) is the well known hinge loss function, and our goal is to\n",
    "perform margin-based linear classification in the RKHS.\n",
    "- K-Means Clustering in Euclidean Space: Let Z = R\n",
    "n\n",
    ", let H be all subsets of R\n",
    "n of size k, and let\n",
    "f(h; z) = minc∈h kc−zk\n",
    "2\n",
    ". Here, each h represents a set of k centroids, and f(·) measures the Euclidean\n",
    "distance squared between an instance z and its nearest centroid, according to the hypothesis h.\n",
    "- Density Estimation: Let Z be a subset of R\n",
    "n\n",
    ", let H be a set of bounded probability densities on Z, and\n",
    "let f(h; z) = −log(h(z)). Here, f(·) is simply the negative log-likelihood of an instance z according\n",
    "to the hypothesis density h. Note that to ensure boundedness of f(·), we need to assume that h(z) is\n",
    "lower bounded by a positive constant for all z ∈ Z.\n",
    "- Stochastic Convex Optimization in Hilbert Spaces: Let Z be an arbitrary measurable set, let H\n",
    "be a closed, convex and bounded subset of a Hilbert space, and let f(h; z) be Lipschitz-continuous\n",
    "and convex w.r.t. its first argument. Here, we want to approximately minimize the objective function\n",
    "Ez∼D [ f(h; z)], where the distribution over Z is unknown, based on an empirical sample z1,..., zm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {
    "f576e388f3c949d183aec2185ae24869": {
     "views": [
      {
       "cell_index": 0
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
