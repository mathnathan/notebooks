{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "\n",
    "- Train on data from certain probability distribution\n",
    "- To be able to generate new data within this probability distribution is must learn the features and characteristics that make up this distribution\n",
    "- “What I cannot create, I do not understand.” — Richard Feynman\n",
    "\n",
    "\n",
    "<img src=\"https://openai.com/assets/research/generative-models/gen-c994c9370597f62edbce64af321e7186c41e8fcf4d7503ea876f8a6bdf901135.svg\">\n",
    "\n",
    "\n",
    "<img src=\"https://openai.com/assets/research/generative-models/gencnn-afe135ff8d2725325a22455a488562b0e1cb7ac6a3f60b3cecb373fd043eb202.svg\">\n",
    "\n",
    "\n",
    "<img src=\"https://openai.com/assets/research/generative-models/learning-gan-ffc4c09e6079283f334b2485ae663a6587d937a45ebc1d8aeac23a67889a3cf5.gif\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features from the Generative Models\n",
    "\n",
    "### Technique 1: Examine the effect of each dimention of the code\n",
    "\n",
    "For example, in the images of 3D faces below we vary one continuous dimension of the code, keeping all others fixed. It's clear from the five provided examples (along each row) that the resulting dimensions in the code capture interpretable dimensions, and that the model has perhaps understood that there are camera angles, facial variations, etc., without having been told that these features exist and are important\n",
    "\n",
    "### Elevation\n",
    "<img src=\"https://openai.com/assets/research/generative-models/infogan-2-069a9ff24c4194a444ba286980a5f2693446c1d8f42c2dc240da05fe48e0378d.jpg\">\n",
    "\n",
    "### Lighting\n",
    "<img src=\"https://openai.com/assets/research/generative-models/infogan-3-20e68c4ad01bd22874596ec9b799f76865e89db6768d412c6d9f6e26e37e6823.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2: Decompose the \"though vector\" in a learned feature basis (<a href=\"http://gabgoh.github.io/ThoughtVectors/\">Goh</a>)\n",
    "\n",
    "<img style=\"float:left\" src=\"http://gabgoh.github.io/ThoughtVectors/c2c.svg\">\n",
    "<img style=\"float:left\" src=\"http://gabgoh.github.io/ThoughtVectors/c2r.svg\">\n",
    "<img style=\"float:left\" src=\"http://gabgoh.github.io/ThoughtVectors/r2c.svg\">\n",
    "<img style=\"float:left\" src=\"http://gabgoh.github.io/ThoughtVectors/r2r.svg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 3: Use a deconvolution on each layer of the encoder to examine the features being learned\n",
    "\n",
    "![](deconv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Current Proposal\n",
    "\n",
    "James' astrocyte data is significantly simpler than the majority of data these techniques were designed to operate on. These may be overkill. I propose we start simple and add complexity as it is needed. \n",
    "\n",
    "### Spatio-temporal Features\n",
    "These are often extracted using two approaches.\n",
    "\n",
    "1. They use recurrent networks to \"remember\" past activity. The effect this has on the data/network/features is quite unclear. In addition  most techniques only use a delay of 1 meaning the impact of different \"features\" of the data at different times may not be captured best.\n",
    "\n",
    "2. Few people have used 3D neural networks. Instead of passing a sinlge image through a 2D convolutional network, they pass 3D volumes through a 3D convolutional network. This has been shown to provide superior predicition/classification on videos. The features extracted are more \"isolated\"\n",
    "\n",
    "![](3dfeatures.png)\n",
    "\n",
    "### Short-term goal\n",
    "I would like to explore the features of a 3D convolutional auto-encoder. I will start simple with only 1 hidden layer. I can examine the features using the 3 techniques outlined above. With my current understanding of the auto-encoders and our data I believe 1, maybe two layers should be sufficient to extract the features necessary. This will be because we are using 3D convolution. Depending on the results, I may use a variational auto-encoder to see if superior results are found.\n",
    "\n",
    "1. Implement 3D convolution auto-encoder (Keras or TensorFlow)\n",
    "2. Implement the 3 tools for extracting and visualizing the features extracted (Software packages available for 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
