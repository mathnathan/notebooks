{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receptive-field dynamics in the central visual pathways\n",
    "### Gregory C. DeAngelis, Izumi Ohzawa and Ralph D. Freeman\n",
    "\n",
    "\n",
    "**Receptive Field (RF)** - The area of the visual space within which the discharge of a neuron can be influenced.\n",
    "\n",
    "Textbooks classically describe the RF in only spatial terms, however RFs are intrinsically a function of both space and time. Thus a comprehensive description of an RF must be described in a spatiotemporal manner.\n",
    "\n",
    "- How are the the spatiotemporal RF profiles calculated? They mention *\"varying the correlation delay (t) in the RF mapping algorithm\"*, but I'm missing some intermediate steps to understand what this algorithm is. \n",
    "- What are simple cells? Complex cells?\n",
    "- Simple cells that demonstrate \"tilted\" spacetime RF profiles, i.e. inseparable, are believed to have directional preference. what are the physical mechanisms underlying these tilted profiles?\n",
    "- Complex cells can also detect motion and velocity. But first order RF profiles can not predict its tuning, second order profiles are required. What are the underlying properties between these Simple and Complex cells that allow them to accomplish the same thing? Answer: Complex cells are built from simple cells\n",
    "- What is white noise analysis? Answer: Don't want to exploit directional significance of input data, so use white noise to see only neuronal response.\n",
    "\n",
    "This notion of \"lagged cells\" gives rise to a rather simple ANN idea to detect the \"hotspots\" of James' calcium data. We construct a recursive CNN where the kernels have recurrent connections that are \"lagged\". For example, say we have two kernels K1 and K2, where K1 has a recurrent connection to K2. K1 will respond to strong centered stimulus, and K2 will respond to a strong circular/doughnut stimulus with a slightly larger radius. If K1 responds strongly, and K2 responds strongly, and K2 gets feedback from K1 then that is a strong indicator of a diffusive event.\n",
    "\n",
    "---\n",
    "\n",
    "# Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis\n",
    "### Quoc V. Le, Will Y. Zou, Serena Y. Yeung, Andrew Y. Ng\n",
    "\n",
    "- There is a growing interest in unsupervised feature learning methods such as Sparse Coding [31, 21, 34], Deep Belief Nets [7] and Stacked Autoencoders [2] because they learn features directly from data and consequently are more generalizable.\n",
    "- Independent Subspace Analysis (ISA) and Independent Component Analysis (ICA) can learn receptive fields similar to the V1 area of visual cortex when applied to static images and the MT area of visual cortex when applied to sequences of images\n",
    "- An advantage of ISA, compared to the more standard ICA algorithm, is that it learns features that are robust to local translation while being selective to frequency, rotation and velocity.\n",
    "- A disadvantage of ISA, as well as ICA, is that it can be very slow to train when the dimension of the input data is large.\n",
    "- Well-known feature detection methods (“interest point detectors”) are Harris3D [16], Cuboids [5] and Hessian [43]. \n",
    "- For descriptors, popular methods are Cuboids [5], HOG/HOF [17], HOG3D [14] and Extended SURF [43].\n",
    "- In a very recent work, Wang et al. showed that there is no universally best hand-engineered feature for all datasets; their finding suggests that learning features directly from the dataset itself may be more advan- tageous.\n",
    "- In many experiments, we found that this invariant property makes ISA perform much better than other simpler methods such as ICA and sparse coding.\n",
    "\n",
    "\n",
    "COOL IDEA: Imposing that a weight matrix has the constraint $W^TW = I$ ensures the features it finds are diverse.\n",
    "\n",
    "---\n",
    "\n",
    "# LEARNING SPARSE, OVERCOMPLETE REPRESENTATIONS OF TIME-VARYING NATURAL IMAGES\n",
    "### Bruno A. Olshatisen\n",
    "\n",
    "- The basis functions that emerge are space-time inseparable functions that resemble the motion-selective receptive fields of simple-cells in mammalian visual cortex.\n",
    "\n",
    "# Spatio-Temporal Convolutional Sparse Auto-Encoder for Sequence Classification\n",
    "### Moez Baccouche, Franck Mamalet, Christian Wolf, Christophe Garcia, Atilla Baskurt\n",
    "\n",
    "- Contrary to the dominant methodology, which relies on hand-crafted features that are manually engineered to be optimal for a specific task, their neural model automatically learns a sparse shift-invariant representation of the local 2D+t salient information, without any use of prior knowledge.\n",
    "\n",
    "- However, several recent works advocate the use of sparse-overcomplete representations, i.e. whose dimension is larger than the input one, but where only a small number of components are non-zero. Several sparsifying procedures have been presented in the literature, including the one proposed by Ranzato et al.\n",
    "\n",
    "- They use some strange **sparsifying logistic** between the encoder and the decoder. I do not understand this idea...\n",
    "\n",
    "- I did not finish it, but in general I do not like this paper\n",
    "\n",
    "\n",
    "# Wilfredo's Research\n",
    "\n",
    "- Immature neurons have more chloride being pumped in than being pumped out. So when GABA binds, chloride flows out hyperpolarizing the cell. During aging, this switches and more chloride is pumped out than is pumped in. Then when GABA binds, chlorida flows in and depolarizes the cell. This explains how GABAergic neurons are excitatory in adolescents and inhibitory in adults.\n",
    "\n",
    "# Learning to learn by gradient descent by gradient descent\n",
    "### Google DeepMind\n",
    "\n",
    "- They pose a general optimization problem for a specific task as a learning problem. Then they learn the best optimization algorithm. Their results show impressive improvements over traditional optimization algorithms.\n",
    "\n",
    "# UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS\n",
    "### Alec Radford, Luke Metz, Soumith Chintala\n",
    "\n",
    "- They introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.\n",
    "\n",
    "- Brush up on variational Bayes Methods: https://en.wikipedia.org/wiki/Variational_Bayesian_methods\n",
    "\n",
    "- “What I cannot create, I do not understand.” —Richard FeynmanY\n",
    "\n",
    "- Research more on Kullback–Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
